{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from typing import Dict, Tuple, List\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from evaluation import evaluation\n",
    "import evaluation\n",
    "from model import Distmult, Complex, Conve, Transe\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-benefit",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pseudocode - \n",
    "    - Load the poisoned dataset, test.txt is the file with target triples, influential_triples.txt has influential triples\n",
    "    - (but need to load the target triples from target dataset to get correct to_skip_eval; otherwise can regenerate the dicts)\n",
    "    - Load the original model and compute ranks on target triples\n",
    "    - Load the poisoned model and compute ranks on target triples \n",
    "    - Compute the difference in original and poisoned ranks\n",
    "    - Sort the indexes of target triples based on the difference in their ranks\n",
    "    - identify the influential triple for highest rank diff and lowest rank diff\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s - Â  %(message)s',\n",
    "                            datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                            level = logging.INFO,\n",
    "                            #filename = log_path\n",
    "                           )\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set arguments to pass to model init later\n",
    "parser = utils.get_argument_parser()\n",
    "sys.argv = ['prog.py']\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-massage",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model = 'distmult'\n",
    "args.original_data = 'FB15k-237'\n",
    "attack_method = 'if'\n",
    "args.data = '{}_del_{}_{}_0_100_1_1_1'.format(attack_method, args.model, args.original_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the hyperparams\n",
    "args = utils.set_hyperparams(args)\n",
    "\n",
    "## set the device - legacy code to re-use functions from utils\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Model name: {}\\n'.format(args.model))\n",
    "logger.info('Dataset name: {} \\n'.format(args.data))\n",
    "logger.info('Original dataset name: {} \\n'.format(args.original_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the target dataset and coresponding eval dictionaries\n",
    "logger.info('------------ Load the target dataset ----------')\n",
    "data_path = 'data/target_{}_{}_0_100_1'.format(args.model, args.original_data)\n",
    "\n",
    "n_ent, n_rel, ent_to_id, rel_to_id = utils.generate_dicts(data_path)\n",
    "\n",
    "data  = utils.load_data(data_path)\n",
    "train_data, valid_data, test_data = data['train'], data['valid'], data['test']\n",
    "\n",
    "inp_f = open(os.path.join(data_path, 'to_skip_eval.pickle'), 'rb')\n",
    "to_skip_eval: Dict[str, Dict[Tuple[int, int], List[int]]] = pickle.load(inp_f)\n",
    "inp_f.close()\n",
    "to_skip_eval['lhs'] = {(int(k[0]), int(k[1])): v for k,v in to_skip_eval['lhs'].items()}\n",
    "to_skip_eval['rhs'] = {(int(k[0]), int(k[1])): v for k,v in to_skip_eval['rhs'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "## example name of original model\n",
    "## FB15k-237_distmult_200_0.2_0.3_0.3.model\n",
    "\n",
    "## example name of poisoned model\n",
    "## cos_del_distmult_FB15k-237_0_100_1_1_1_distmult_200_0.2_0.3_0.3.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('-------- Load the original model -----------')\n",
    "## set the model path without hyperparam arguments\n",
    "model_dir = 'saved_models/{}_{}_*.model'.format(args.original_data, args.model)\n",
    "for filename in glob.glob(model_dir):\n",
    "    model_path = filename\n",
    "    \n",
    "# add a model and load the pre-trained params\n",
    "original_model = utils.load_model(model_path, args, n_ent, n_rel, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-advice",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('------- Ranks on target dataset from original model ----------')\n",
    "### legacy code\n",
    "if args.add_reciprocals:\n",
    "    num_rel= n_rel\n",
    "else:\n",
    "    num_rel = 0\n",
    "    \n",
    "test_data = torch.from_numpy(test_data.astype('int64')).to(device)\n",
    "ranks_lhs, ranks_rhs = evaluation.get_ranking(original_model, test_data, num_rel, to_skip_eval, device)\n",
    "ranks_lhs, ranks_rhs = np.array(ranks_lhs), np.array(ranks_rhs)\n",
    "ranks = np.mean( np.array([ ranks_lhs, ranks_rhs ]), axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_lhs = np.mean(ranks_lhs, dtype=np.float64)\n",
    "mr_rhs = np.mean(ranks_rhs, dtype=np.float64)\n",
    "mr = np.mean(ranks, dtype=np.float64)\n",
    "### these should match the mean values from log files\n",
    "logger.info('Original mean ranks. Lhs:{}, Rhs:{}, Mean:{}\\n'.format(mr_lhs, mr_rhs, mr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the poisoned dataset and coresponding eval dictionaries\n",
    "logger.info('------------ Load the poisoned dataset ----------')\n",
    "data_path = 'data/{}'.format(args.data)\n",
    "\n",
    "n_ent, n_rel, ent_to_id, rel_to_id = utils.generate_dicts(data_path)\n",
    "\n",
    "data  = utils.load_data(data_path)\n",
    "train_data, valid_data, test_data = data['train'], data['valid'], data['test']\n",
    "\n",
    "inp_f = open(os.path.join(data_path, 'to_skip_eval.pickle'), 'rb')\n",
    "to_skip_eval: Dict[str, Dict[Tuple[int, int], List[int]]] = pickle.load(inp_f)\n",
    "inp_f.close()\n",
    "to_skip_eval['lhs'] = {(int(k[0]), int(k[1])): v for k,v in to_skip_eval['lhs'].items()}\n",
    "to_skip_eval['rhs'] = {(int(k[0]), int(k[1])): v for k,v in to_skip_eval['rhs'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "# influential triples\n",
    "inf_df = pd.read_csv(os.path.join(data_path, 'influential_triples.txt'), sep='\\t', header=None, names=None, dtype=int)\n",
    "inf_data = inf_df.values\n",
    "del inf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('-------- Load the poisoned model -----------')\n",
    "## set the model path without hyperparam arguments\n",
    "model_dir = 'saved_models/{}_{}_*.model'.format(args.data, args.model)\n",
    "for filename in glob.glob(model_dir):\n",
    "    model_path = filename\n",
    "    \n",
    "# add a model and load the pre-trained params\n",
    "poisoned_model = utils.load_model(model_path, args, n_ent, n_rel, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-fleece",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('------- Ranks on target dataset from poisoned model ----------')\n",
    "logger.info('(using eval dicts from poisoned data)')\n",
    "\n",
    "### legacy code\n",
    "if args.add_reciprocals:\n",
    "    num_rel= n_rel\n",
    "else:\n",
    "    num_rel = 0\n",
    "    \n",
    "test_data = torch.from_numpy(test_data.astype('int64')).to(device)\n",
    "pos_ranks_lhs, pos_ranks_rhs = evaluation.get_ranking(poisoned_model, test_data, num_rel, to_skip_eval, device)\n",
    "pos_ranks_lhs, pos_ranks_rhs = np.array(pos_ranks_lhs), np.array(pos_ranks_rhs)\n",
    "pos_ranks = np.mean( np.array([ pos_ranks_lhs, pos_ranks_rhs ]), axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-queue",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_mr_lhs = np.mean(pos_ranks_lhs, dtype=np.float64)\n",
    "pos_mr_rhs = np.mean(pos_ranks_rhs, dtype=np.float64)\n",
    "pos_mr = np.mean(pos_ranks, dtype=np.float64)\n",
    "### these should match the mean values from log files\n",
    "logger.info('Poisoned mean ranks. Lhs:{}, Rhs:{}, Mean:{}\\n'.format(pos_mr_lhs, pos_mr_rhs, pos_mr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-george",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-calvin",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_diff = pos_ranks - ranks\n",
    "sorted_idx = np.argsort(ranks_diff) ## indices of sorted ranks\n",
    "sorted_diffs = ranks_diff[sorted_idx] ## values of sorted ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    if test_data.is_cuda:\n",
    "        test_data = test_data.cpu().numpy() #remove the torch tensor\n",
    "except:\n",
    "    test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the entities from IDs\n",
    "id_to_ent = {ent_to_id[k]:k for k in ent_to_id.keys()}\n",
    "id_to_rel = {rel_to_id[k]:k for k in rel_to_id.keys()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_s, max_p, max_o = test_data[sorted_idx[-1]]\n",
    "max_h, max_r, max_t = inf_data[sorted_idx[-1]]\n",
    "\n",
    "min_s, min_p, min_o = test_data[sorted_idx[0]]\n",
    "min_h, min_r, min_t = inf_data[sorted_idx[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_target = [id_to_ent[max_s], id_to_rel[max_p], id_to_ent[max_o]]\n",
    "max_inf = [id_to_ent[max_h], id_to_rel[max_r], id_to_ent[max_t]]\n",
    "\n",
    "min_target = [id_to_ent[min_s], id_to_rel[min_p], id_to_ent[min_o]]\n",
    "min_inf = [id_to_ent[min_h], id_to_rel[min_r], id_to_ent[min_t]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('---- For {} on {} {}\\n'.format(attack_method, args.model, args.original_data))\n",
    "\n",
    "logger.info('Maximum change in ranks: {}\\n'.format(sorted_diffs[-1]))\n",
    "logger.info('Target triple with maximum change: {}\\n'.format(max_target))\n",
    "logger.info('Corresponding influential triple: {}\\n'.format(max_inf))\n",
    "\n",
    "logger.info('Minimum change in ranks: {}\\n'.format(sorted_diffs[0]))\n",
    "logger.info('Target triple with minimum change: {}\\n'.format(min_target))\n",
    "logger.info('Corresponding influential triple: {}\\n'.format(min_inf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-diabetes",
   "metadata": {},
   "source": [
    "use this to change Freebase IDs to values\n",
    "\n",
    "Link - https://freebase.toolforge.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-jefferson",
   "metadata": {},
   "source": [
    "Another method is to use the Google Knowledge Graph Search API\n",
    "\n",
    "Link - https://developers.google.com/knowledge-graph/reference/rest/v1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-language",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hungry-decline",
   "metadata": {},
   "source": [
    "Original WN18RR dataset with definition files (to get entity values from IDs) - \n",
    "- Link1 - https://figshare.com/articles/dataset/WN18/11869548/2\n",
    "- Link2 - https://everest.hds.utc.fr/doku.php?id=en:smemlj12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-monroe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-dining",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
